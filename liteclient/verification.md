*** This is the beginning of an unfinished draft. Don't continue reading! ***

# Core Verification

<< Rough outline of what the component is doing and why. 2-3 paragraphs >>

The lite client implements a read operation of a block (header) from the
blockchain. It does so by communicating with full nodes. As full nodes
may be faulty, this functionality must be implemented in a fault-tolerant
way.



# Part I - Outside view

## Context of this document

<< mention other components and or specifications that are relevant for this
spec. Possible interactions, possible use cases, etc. >>

In this specification we specify the communication of the lite client with
full nodes. As full nodes may be faulty, the lite client has to check whether
the header it receives are the ones generated by Tendermint consensus. The
central features used in this specification are:

 - Tendermint blocks
(and headers) extensively use digital signatures and hashes to prove that a
block is OK.

 - the Tendermint failure model that guarantees that there is a set of full nodes
 that represent more than 2/3 of the
 voting power in the "next validators" set
 are correct from the time a block is generated until the trusting period is
 passed.

<< should give the reader the understanding in what environment this component
will be used. >>

## Informal Problem statement

Given a height *h* as an input, the lite client outputs the header of
height *h* that is generated by Tendermint consensus.


## Sequential Problem statement

The blockchain is a list of headers with strictly increasing heights.

If the blockchain contains a header of height *h*, then for all $h'<h$
it contains a header.

During operation, new headers may be appended to the list.

The lite client gets as input an integer, and eventually outputs the header
of height *h*.

# Part II - Protocol view

## Environment/Assumptions/Incentives

*** fix below paragraph *** 
It is not in the interest of faulty full nodes to talk to the failure detector as long as the failure detector is connected to at least one correct full node. This would only increase the likelihood of misbehavior being detected. Also we cannot punish them easily (cheaply). The absence of a response need not be the fault of the full node. Also, a faulty failure detector could wrongly accuse correct full nodes.

Correct full nodes have the incentive to respond, because the failure detector may help them to understand whether their header is a good one. We can thus base liveness arguments of the failure detector on the assumptions that correct full nodes reliably talk to us.


**Assumptions**

- **A1.** At all times there is at least one correct full node among the primary and the secondary.

- **A2.** Communication between the failure detector and a correct full node is reliable and bounded in time.



<< Introduce distributed aspects >>

<< Timing and correctness assumptions. Possibly with justification that the
assumptions make sense, e.g., it is in the interest of a full node to behave
correctly >>

## Problem Statement

<< safety specifications / invariants in English >>

<< liveness specifications in English. Possibly with timing/fairness requirements:
e.g., if the component is connected to a correct full node and communication is
reliable and timely, then something good happens eventually. >>

should have clear formalization in temporal logic.

## Definitions

In this section we become more concrete, with basic data types,

some math that allows to write specifications and pseudo code solution below.
Some variables, etc.

## Solution

<< Basic data structures. Simplified, so that we can focus on the distributed
algorithm here. If existing: link to Tendermint data structures, and mentioned
if details were omitted. >>

<< Pseudo code of the solution >>


## Correctness arguments

<< Proof sketches of why we believe the solution satisfies the specifications.
Possibly giving inductive invariants that can be used to prove the specifications >>
<<Link to Part I>>
